# 20251027-20251102

## 2025-10-28

- **[arXiv2510] TokenTiming: A Dynamic Alignment Method for Universal Speculative
  Decoding Model Pairs**
  - **tags:** [mlsys], [LLM inference], [speculative decoding, vocabulary alignment, dynamic time warping, token re-encoding, probability distribution transfer]
  - **authors:** Sibo Xiao, Jinyuan Fu, Zhongle Xie, Lidan Shou
  - **institution:** Zhejiang University
  - **link:** http://arxiv.org/pdf/2510.15545v2
  - **Simple LLM Summary:** TokenTiming introduces a dynamic alignment method using Dynamic Time Warping to enable speculative decoding between models with mismatched vocabularies. It re-encodes draft token sequences and transfers probability distributions without requiring model retraining. The approach achieves 1.57x speedup and makes draft model selection more flexible for LLM acceleration.

- **[arXiv2510] CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement
  Learning**
  - **tags:** [mlsys], [kernels], [CUDA optimization, reinforcement learning, contrastive learning, GPU computing, automated code optimization]
  - **authors:** Xiaoya Li, Xiaofei Sun, Albert Wang, Jiwei Li, Chris Shum
  - **institution:** DeepReinforce
  - **link:** http://arxiv.org/pdf/2507.14111v8
  - **Simple LLM Summary:** CUDA-L1 introduces a contrastive reinforcement learning framework that transforms LLMs into effective CUDA optimizers using speedup-based rewards. The method achieves significant performance improvements across multiple GPU architectures, with average speedups of 3.12x on A100. It autonomously discovers and combines optimization techniques while identifying performance bottlenecks without requiring human expertise.

## 2025-10-29

- **[arXiv2510] The AI_INFN Platform: Artificial Intelligence Development in the Cloud**
  - **tags:** [mlsys], [cluster infrastructure], [Kubernetes platform, GPU acceleration, distributed computing, cloud-native technologies, Virtual Kubelet, InterLink API, workflow management]
  - **authors:** Lucio Anderlini, Giulio Bianchini, Diego Ciangottini, Stefano Dal Pra, Diego Michelotto, Rosa Petrini, Daniele Spiga
  - **institution:** Istituto Nazionale di Fisica Nucleare (INFN)
  - **link:** http://arxiv.org/pdf/2509.22117v2
  - **Simple LLM Summary:** The paper presents a Kubernetes-based platform deployed on INFN Cloud to simplify GPU-powered data analysis workflows and enable scalable execution across heterogeneous distributed resources. By integrating offloading mechanisms through Virtual Kubelet and InterLink API, the platform successfully spans multiple resource providers including WLCG sites and HPC centers. Preliminary benchmarks and case studies demonstrate effective performance and integration outcomes for AI/ML workflows in scientific research.

- **[arXiv2510] Plexus: Taming Billion-edge Graphs with 3D Parallel Full-graph GNN
  Training**
  - **tags:** [mlsys], [Other models training], [graph neural networks, parallel computing, full-graph training, load balancing, performance modeling]
  - **authors:** Aditya K. Ranjan, Siddharth Singh, Cunyang Wei, Abhinav Bhatele
  - **institution:** University of Maryland
  - **link:** http://arxiv.org/pdf/2505.04083v2
  - **Simple LLM Summary:** Plexus introduces a 3D parallel approach for full-graph GNN training that addresses communication overhead and load imbalance issues in billion-edge graphs. The method includes optimizations like double permutation for load balancing and a performance model to determine optimal configurations. Evaluation shows significant speedups of 2.3-12.5x over prior work and reduces time-to-solution by 5.2-54.2x on supercomputers.
