# 20251027-20251102

## 2025-10-28

- **[arXiv2510] TokenTiming: A Dynamic Alignment Method for Universal Speculative
  Decoding Model Pairs**
  - **tags:** [mlsys], [LLM inference], [speculative decoding, vocabulary alignment, dynamic time warping, token re-encoding, probability distribution transfer]
  - **authors:** Sibo Xiao, Jinyuan Fu, Zhongle Xie, Lidan Shou
  - **institution:** Zhejiang University
  - **link:** http://arxiv.org/pdf/2510.15545v2
  - **Simple LLM Summary:** TokenTiming introduces a dynamic alignment method using Dynamic Time Warping to enable speculative decoding between models with mismatched vocabularies. It re-encodes draft token sequences and transfers probability distributions without requiring model retraining. The approach achieves 1.57x speedup and makes draft model selection more flexible for LLM acceleration.

- **[arXiv2510] CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement
  Learning**
  - **tags:** [mlsys], [kernels], [CUDA optimization, reinforcement learning, contrastive learning, GPU computing, automated code optimization]
  - **authors:** Xiaoya Li, Xiaofei Sun, Albert Wang, Jiwei Li, Chris Shum
  - **institution:** DeepReinforce
  - **link:** http://arxiv.org/pdf/2507.14111v8
  - **Simple LLM Summary:** CUDA-L1 introduces a contrastive reinforcement learning framework that transforms LLMs into effective CUDA optimizers using speedup-based rewards. The method achieves significant performance improvements across multiple GPU architectures, with average speedups of 3.12x on A100. It autonomously discovers and combines optimization techniques while identifying performance bottlenecks without requiring human expertise.

## 2025-10-29

- **[arXiv2510] The AI_INFN Platform: Artificial Intelligence Development in the Cloud**
  - **tags:** [mlsys], [cluster infrastructure], [Kubernetes platform, GPU acceleration, distributed computing, cloud-native technologies, Virtual Kubelet, InterLink API, workflow management]
  - **authors:** Lucio Anderlini, Giulio Bianchini, Diego Ciangottini, Stefano Dal Pra, Diego Michelotto, Rosa Petrini, Daniele Spiga
  - **institution:** Istituto Nazionale di Fisica Nucleare (INFN)
  - **link:** http://arxiv.org/pdf/2509.22117v2
  - **Simple LLM Summary:** The paper presents a Kubernetes-based platform deployed on INFN Cloud to simplify GPU-powered data analysis workflows and enable scalable execution across heterogeneous distributed resources. By integrating offloading mechanisms through Virtual Kubelet and InterLink API, the platform successfully spans multiple resource providers including WLCG sites and HPC centers. Preliminary benchmarks and case studies demonstrate effective performance and integration outcomes for AI/ML workflows in scientific research.

- **[arXiv2510] Plexus: Taming Billion-edge Graphs with 3D Parallel Full-graph GNN
  Training**
  - **tags:** [mlsys], [Other models training], [graph neural networks, parallel computing, full-graph training, load balancing, performance modeling]
  - **authors:** Aditya K. Ranjan, Siddharth Singh, Cunyang Wei, Abhinav Bhatele
  - **institution:** University of Maryland
  - **link:** http://arxiv.org/pdf/2505.04083v2
  - **Simple LLM Summary:** Plexus introduces a 3D parallel approach for full-graph GNN training that addresses communication overhead and load imbalance issues in billion-edge graphs. The method includes optimizations like double permutation for load balancing and a performance model to determine optimal configurations. Evaluation shows significant speedups of 2.3-12.5x over prior work and reduces time-to-solution by 5.2-54.2x on supercomputers.

## 2025-10-30

- **[arXiv2510] DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World
  Serving**
  - **tags:** [mlsys], [LLM inference], [speculative decoding, dynamic adaptation, KLD stability, large-batch serving, latency optimization]
  - **authors:** Mingyu Yang, Jae-Young Choi, Kihyo Moon, Minsung Jang, Eunjoo Jeon
  - **institution:** Samsung SDS
  - **link:** http://arxiv.org/pdf/2509.01083v3
  - **Simple LLM Summary:** DSDE introduces a training-free framework that uses Kullback-Leibler divergence variance as a stability signal to dynamically adjust speculation length during LLM inference. This approach achieves competitive latency and superior robustness across diverse workloads, especially in low-acceptance-rate scenarios. The method validates post-hoc signals as valuable components for building more intelligent and adaptive LLM inference systems.

- **[arXiv2510] TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM
  Inference**
  - **tags:** [mlsys], [LLM inference], [distributed inference, communication optimization, kernel fusion, compute-communication overlap, token splitting]
  - **authors:** Raja Gond, Nipun Kwatra, Ramachandran Ramjee
  - **institution:** Microsoft Research India
  - **link:** http://arxiv.org/pdf/2505.11329v4
  - **Simple LLM Summary:** TokenWeave introduces token-splitting to divide inference batches into subsets and overlaps communication of one subset with computation of another. It also implements a fused AllReduce-RMSNorm kernel that leverages Multimem instructions on NVIDIA GPUs. The approach achieves up to 1.29x latency speedup and 1.26x higher throughput while reducing SM usage for communication operations.

- **[arXiv2510] MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning**
  - **tags:** [mlsys], [LLM training], [memory optimization, SSD offloading, fine-tuning, system memory fragmentation, pinned buffer allocation]
  - **authors:** Yong-Cheng Liaw, Shuo-Han Chen
  - **institution:** National Taiwan University
  - **link:** http://arxiv.org/pdf/2505.23254v3
  - **Simple LLM Summary:** MemAscend introduces a framework that optimizes system memory usage in SSD-offloaded LLM fine-tuning by addressing memory fragmentation, inefficient pinned buffer allocation, and peak CPU usage. The method reduces peak system memory consumption by 55.7% on average compared to standard techniques. This enables larger models and higher batch sizes on resource-constrained hardware.

- **[arXiv2510] GPU-Accelerated Primal Heuristics for Mixed Integer Programming**
  - **tags:** [mlsys], [kernels], [GPU acceleration, mixed integer programming, primal heuristics, optimization algorithms, parallel computing]
  - **authors:** Akif Çördük, Piotr Sielski, Alice Boucher, Kumar Aatish
  - **institution:** Nvidia
  - **link:** http://arxiv.org/pdf/2510.20499v2
  - **Simple LLM Summary:** This paper presents GPU-accelerated primal heuristics for Mixed Integer Programming, combining several state-of-the-art methods including Feasibility Pump and Fix-and-Propagate with GPU parallelization. The approach uses a GPU-accelerated PDLP as an approximate LP solver and introduces a probing cache for faster operations. The method achieves significant improvements, obtaining 221 feasible solutions with 22% objective gap on the MIPLIB2017 benchmark.

- **[arXiv2510] AIMeter: Measuring, Analyzing, and Visualizing Energy and Carbon
  Footprint of AI Workloads**
  - **tags:** [mlsys], [Other models training, Other models inference], [energy measurement, carbon footprint, AI sustainability, performance analysis, Green AI]
  - **authors:** Hongzhen Huang, Kunming Zhang, Hanlong Liao, Kui Wu, Guoming Tang
  - **institution:** The Hong Kong University of Science and Technology (Guangzhou), National University of Defense Technology, University of Victoria
  - **link:** http://arxiv.org/pdf/2506.20535v2
  - **Simple LLM Summary:** AIMeter is a software toolkit that measures, analyzes and visualizes energy consumption, power draw and carbon emissions across AI workloads. It integrates with existing AI frameworks to provide standardized reports and fine-grained time-series data for benchmarking. The tool enables correlation analysis between hardware metrics and model performance to promote sustainable AI practices.

- **[arXiv2510] Lost in Tokenization: Context as the Key to Unlocking Biomolecular
  Understanding in Scientific LLMs**
  - **tags:** [mlsys], [LLM inference], [scientific LLMs, biomolecular sequences, tokenization, structured context, bioinformatics, biological reasoning]
  - **authors:** Kai Zhuang, Jiawei Zhang, Yumou Liu, Hanqun Cao, Chunbin Gu, Mengdi Liu, Zhangyang Gao, Zitong Jerry Wang, Xuanhe Zhou, Pheng-Ann Heng, Lijun Wu, Conghui He, Cheng Tan
  - **institution:** Shanghai Artificial Intelligence Laboratory, Westlake University, Shanghai Innovation Institute, Shanghai Jiaotong University, The Chinese University of Hong Kong, Institute of Computing Technology, Chinese Academy of Sciences
  - **link:** http://arxiv.org/pdf/2510.23127v2
  - **Simple LLM Summary:** This paper proposes using high-level structured context from bioinformatics tools instead of raw biomolecular sequences as input to Scientific LLMs. Through systematic experiments comparing sequence-only, context-only, and combined inputs, they found context-only approach consistently outperforms others. The study concludes that Sci-LLMs function better as reasoning engines over expert knowledge rather than sequence decoders.

- **[arXiv2510] Analysis and Optimized CXL-Attached Memory Allocation for Long-Context
  LLM Fine-Tuning**
  - **tags:** [mlsys], [finetuning], [CXL memory, memory allocation, CPU offloading, PyTorch extension, long-context LLM]
  - **authors:** Yong-Cheng Liaw, Shuo-Han Chen
  - **institution:** National Taiwan University
  - **link:** http://arxiv.org/pdf/2507.03305v2
  - **Simple LLM Summary:** This paper introduces a PyTorch extension with CXL-aware memory allocator that enables fine-grained tensor placement, keeping latency-critical data in local DRAM while striping tolerant data across CXL devices. The approach achieves 97-99% of DRAM-only performance with single CXL add-in card and near-100% with two cards. Results demonstrate CXL-attached memory as a practical solution for scaling long-context LLM fine-tuning beyond DRAM capacity limits.
