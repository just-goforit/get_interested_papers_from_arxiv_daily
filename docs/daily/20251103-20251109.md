# 20251103-20251109

## 2025-11-05

- **[arXiv2511] NOWS: Neural Operator Warm Starts for Accelerating Iterative Solvers**
  - **tags:** [mlsys], [Other models inference], [PDE solvers, neural operators, iterative methods, Krylov methods, computational acceleration]
  - **authors:** Mohammad Sadegh Eshaghi, Cosmin Anitescu, Navid Valizadeh, Yizheng Wang, Xiaoying Zhuang, Timon Rabczuk
  - **institution:** Leibniz University Hannover, Bauhaus-Universität Weimar, Tsinghua University
  - **link:** http://arxiv.org/pdf/2511.02481v2
  - **Simple LLM Summary:** The paper introduces NOWS, a hybrid approach that uses neural operators to generate high-quality initial guesses for classical iterative PDE solvers. This warm-start strategy significantly reduces iteration counts and computational time by up to 90% while maintaining solver stability and convergence guarantees. The method integrates seamlessly with existing numerical discretizations and preserves the rigor of traditional solvers.

## 2025-11-06

- **[arXiv2511] GPUnion: Autonomous GPU Sharing on Campus**
  - **tags:** [mlsys], [cluster infrastructure, scheduling], [GPU sharing, containerization, resource management, checkpointing, migration]
  - **authors:** Yufang Li, Yuanbo Zhang, Hanlong Liao, Deke Guo, Guoming Tang
  - **institution:** HKUST(GZ), Sun Yat-sen University
  - **link:** http://arxiv.org/pdf/2507.18928v2
  - **Simple LLM Summary:** GPUnion introduces a campus-scale GPU sharing platform using container-based task dispatching, provider-first architecture, and resilient execution with automatic checkpointing and migration. The system demonstrates 30% higher GPU utilization, 40% more interactive sessions, and 94% successful workload migration during provider departures, enabling voluntary participation while preserving provider autonomy.

- **[arXiv2511] SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based
  Model Inference**
  - **tags:** [mlsys], [LLM inference], [speculative decoding, mixture-of-experts, expert prefetching, model offloading, inference acceleration]
  - **authors:** Liangkun Chen, Zijian Wen, Tian Wu, Xiaoxi Zhang, Chuan Wu
  - **institution:** Sun Yat-sen University, The University of Hong Kong
  - **link:** http://arxiv.org/pdf/2510.10302v2
  - **Simple LLM Summary:** SP-MoE introduces speculative expert prefetching and compute-communication pipelining to accelerate MoE-based model inference. The framework uses structural correspondence between draft and target models to prefetch experts ahead of verification. Experiments show 1.07-3.5× speedup over state-of-the-art methods across diverse datasets and models.

- **[arXiv2511] Arrow: Adaptive Scheduling Mechanisms for Disaggregated LLM Inference
  Architecture**
  - **tags:** [mlsys], [scheduling], [LLM inference, adaptive scheduling, resource disaggregation, prefill-decode architecture, load balancing]
  - **authors:** Yu Wu, Tongxuan Liu, Yuting Zeng, Siyu Wu, Jun Xiong, Xianzhe Dong, Hailong Yang, Ke Zhang, Jing Li
  - **institution:** University of Science and Technology of China, Beihang University, JD.com
  - **link:** http://arxiv.org/pdf/2505.11916v2
  - **Simple LLM Summary:** Arrow introduces an adaptive scheduler that dynamically adjusts prefill and decode instances based on real-time cluster metrics using stateless instances and latency characteristics. It addresses computational load imbalances caused by varying request lengths in disaggregated LLM serving architectures. Evaluation shows Arrow achieves up to 2.55× higher request serving rates compared to state-of-the-art systems.
